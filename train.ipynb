{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "from maps.SumoEnv import SumoEnv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumoDQNAgent:\n",
    "    def __init__(self, action_space_n, observation_space_n, config):\n",
    "        self.action_space_n = action_space_n\n",
    "        self.observation_space_n = observation_space_n\n",
    "        self.config = config\n",
    "        self.device = self.set_device()\n",
    "        self.set_random_seed(config['random_seed'])\n",
    "\n",
    "        # Define the neural networks\n",
    "        self.model = self.create_model()\n",
    "        self.target_model = copy.deepcopy(self.model)\n",
    "\n",
    "        # Initialize environment and state matrices\n",
    "        self.env = SumoEnv(gui=False, flow_on_HW=config['flow_on_HW'], flow_on_Ramp=config['flow_on_Ramp'])\n",
    "        self.state_matrices = deque(maxlen=config['state_queue_len'])\n",
    "        self.initialize_state_matrices()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=config['learning_rate'])\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.replay = deque(maxlen=config['mem_size'])\n",
    "        \n",
    "        # Traffic flow data (convert times to steps assuming 1 step = 1 second)\n",
    "        self.data_points = [(t * 60, hw, ramp) for t, hw, ramp in config['data_points']]\n",
    "        \n",
    "    def set_device(self):\n",
    "        device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {device}\")\n",
    "        return device\n",
    "\n",
    "    def set_random_seed(self, seed):\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "    def create_model(self):\n",
    "        \"\"\"Defines the neural network architecture.\"\"\"\n",
    "        layers = [self.observation_space_n, 128, 64, 32, 8, self.action_space_n]\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(layers[0], layers[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layers[1], layers[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layers[2], layers[3]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layers[3], layers[4]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layers[4], layers[5])\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def initialize_state_matrices(self):\n",
    "        \"\"\"Initializes the deque with zero state matrices.\"\"\"\n",
    "        for _ in range(self.config['state_queue_len']):\n",
    "            state_matrix = np.zeros((4, 251))\n",
    "            self.state_matrices.appendleft(state_matrix)\n",
    "\n",
    "    def obs(self):\n",
    "        \"\"\"Converts state matrices into a flat tensor for model input.\"\"\"\n",
    "        state_matrix = self.env.getStateMatrixV2()\n",
    "        self.state_matrices.appendleft(state_matrix)\n",
    "        flat_state_array = np.concatenate(self.state_matrices).flatten()\n",
    "        return torch.from_numpy(flat_state_array).float()\n",
    "\n",
    "    def rew(self):\n",
    "        \"\"\"Calculates the reward using configured weights.\"\"\"\n",
    "        return (\n",
    "            self.config['mu'] * self.env.getSpeedHW() + \n",
    "            self.config['omega'] * self.env.getNumberVehicleWaitingTL() + \n",
    "            self.config['tau'] * self.env.getSpeedRamp()\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the environment and state matrix.\"\"\"\n",
    "        self.initialize_state_matrices()\n",
    "        self.env.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Executes action and adjusts flow dynamically in the simulation.\"\"\"\n",
    "        for _ in range(self.config['simulation_step_len']):\n",
    "            self.env.setFlowOnHW(self.interpolate_flow(self.env.getCurrentStep(), self.data_points)[0])\n",
    "            self.env.setFlowOnRamp(self.interpolate_flow(self.env.getCurrentStep(), self.data_points)[1])\n",
    "            self.env.doSimulationStep(action)\n",
    "\n",
    "    def interpolate_flow(self, step, data_points):\n",
    "        \"\"\"Performs linear interpolation of the flow data.\"\"\"\n",
    "        times, hw_flows, ramp_flows = zip(*data_points)\n",
    "        hw_flow = np.interp(step, times, hw_flows)\n",
    "        ramp_flow = np.interp(step, times, ramp_flows)\n",
    "        return int(hw_flow), int(ramp_flow)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Trains the model using DQN with experience replay.\"\"\"\n",
    "        total_loss, total_rewards = [], []\n",
    "        total_steps = 0\n",
    "        \n",
    "        for epoch in range(self.config['epochs']):\n",
    "            print(\"Epoch:\", epoch)\n",
    "            epsilon = self.update_epsilon(epoch)\n",
    "            self.reset()\n",
    "            state1 = self.obs()\n",
    "            is_done = False\n",
    "            steps = 0\n",
    "            \n",
    "            while not is_done:\n",
    "                total_steps += 1\n",
    "                steps += 1\n",
    "                action = self.select_action(state1, epsilon)\n",
    "                self.step(action)\n",
    "                state2 = self.obs()\n",
    "                reward = self.rew()\n",
    "                total_rewards.append(reward)\n",
    "                \n",
    "                self.replay.append((state1, action, reward, state2, is_done))\n",
    "                state1 = state2\n",
    "                \n",
    "                if len(self.replay) > self.config['batch_size']:\n",
    "                    loss = self.replay_experience()\n",
    "                    total_loss.append(loss.item())\n",
    "\n",
    "                    if total_steps % self.config['sync_freq'] == 0:\n",
    "                        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "                is_done = (steps >= self.config['max_steps'])\n",
    "        \n",
    "        return self.model, np.array(total_loss), np.array(total_rewards)\n",
    "\n",
    "    def select_action(self, state, epsilon):\n",
    "        \"\"\"Selects an action using epsilon-greedy policy.\"\"\"\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, self.action_space_n - 1)\n",
    "        with torch.no_grad():\n",
    "            return torch.argmax(self.model(state)).item()\n",
    "\n",
    "    def replay_experience(self):\n",
    "        \"\"\"Trains model on a random minibatch from replay memory.\"\"\"\n",
    "        minibatch = random.sample(self.replay, self.config['batch_size'])\n",
    "        state1_batch = torch.cat([s1.unsqueeze(0) for (s1, a, r, s2, d) in minibatch])\n",
    "        action_batch = torch.Tensor([a for (s1, a, r, s2, d) in minibatch])\n",
    "        reward_batch = torch.Tensor([r for (s1, a, r, s2, d) in minibatch])\n",
    "        state2_batch = torch.cat([s2.unsqueeze(0) for (s1, a, r, s2, d) in minibatch])\n",
    "        done_batch = torch.Tensor([d for (s1, a, r, s2, d) in minibatch])\n",
    "\n",
    "        Q1 = self.model(state1_batch)\n",
    "        with torch.no_grad():\n",
    "            Q2 = self.target_model(state2_batch).max(1)[0]\n",
    "        \n",
    "        Y = reward_batch + self.config['gamma'] * ((1 - done_batch) * Q2)\n",
    "        X = Q1.gather(dim=1, index=action_batch.long().unsqueeze(dim=1)).squeeze()\n",
    "\n",
    "        loss = self.loss_fn(X, Y.detach())\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss\n",
    "\n",
    "    def update_epsilon(self, epoch):\n",
    "        \"\"\"Updates epsilon using exponential or linear decay.\"\"\"\n",
    "        if self.config['eps_dec_exp']:\n",
    "            return self.config['eps_min'] + (self.config['eps_start'] - self.config['eps_min']) * np.exp(-self.config['eps_decay_factor'] * epoch)\n",
    "        else:\n",
    "            decay_rate = (self.config['eps_start'] - self.config['eps_min']) / self.config['epochs']\n",
    "            return max(self.config['eps_min'], self.config['eps_start'] - decay_rate * epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 29\u001b[0m\n\u001b[0;32m      2\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_seed\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m33\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflow_on_HW\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m5000\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmem_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m50000\u001b[39m\n\u001b[0;32m     26\u001b[0m }\n\u001b[0;32m     28\u001b[0m agent \u001b[38;5;241m=\u001b[39m SumoDQNAgent(action_space_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, observation_space_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3012\u001b[39m, config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[1;32m---> 29\u001b[0m model, total_loss, total_rewards \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDynamicModel.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 108\u001b[0m, in \u001b[0;36mSumoDQNAgent.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    106\u001b[0m steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    107\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_action(state1, epsilon)\n\u001b[1;32m--> 108\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m state2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs()\n\u001b[0;32m    110\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrew()\n",
      "Cell \u001b[1;32mIn[2], line 82\u001b[0m, in \u001b[0;36mSumoDQNAgent.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39msetFlowOnHW(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterpolate_flow(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mgetCurrentStep(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_points)[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39msetFlowOnRamp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterpolate_flow(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mgetCurrentStep(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_points)[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdoSimulationStep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\WorkSpace\\StudyAdroad\\Gustave-eiffel course\\RL course\\Dynamic-Traffic-light-management-system-main\\HWRamp\\maps\\SumoEnv.py:97\u001b[0m, in \u001b[0;36mSumoEnv.doSimulationStep\u001b[1;34m(self, phase_index)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# TravelTime (length/mean speed) OLD!!!\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# self.travelTimeHW = traci.edge.getTraveltime(\"HW_beforeRamp\") + traci.edge.getTraveltime(\"HW_Ramp\") + traci.edge.getTraveltime(\"HW_afterRamp\")\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# self.travelTimeRamp =  traci.edge.getTraveltime(\"Ramp_beforeTL\") + traci.edge.getTraveltime(\"Ramp_afterTL\") + traci.edge.getTraveltime(\"HW_Ramp\") + traci.edge.getTraveltime(\"HW_afterRamp\")\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     94\u001b[0m \n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Recording the vehicles that appear on the starting edges\u001b[39;00m\n\u001b[0;32m     96\u001b[0m starting_vehicles_HW \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(traci\u001b[38;5;241m.\u001b[39medge\u001b[38;5;241m.\u001b[39mgetLastStepVehicleIDs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHW_beforeRamp\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m---> 97\u001b[0m starting_vehicles_Ramp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mtraci\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetLastStepVehicleIDs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRamp_beforeTL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Update the start times, if not yet recorded\u001b[39;00m\n\u001b[0;32m    100\u001b[0m new_starters_HW \u001b[38;5;241m=\u001b[39m starting_vehicles_HW \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicle_depart_times_HW\u001b[38;5;241m.\u001b[39mkeys()\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\_edge.py:181\u001b[0m, in \u001b[0;36mEdgeDomain.getLastStepVehicleIDs\u001b[1;34m(self, edgeID)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetLastStepVehicleIDs\u001b[39m(\u001b[38;5;28mself\u001b[39m, edgeID):\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;124;03m\"\"\"getLastStepVehicleIDs(string) -> list(string)\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m    Returns the ids of the vehicles for the last time step on the given edge.\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getUniversal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLAST_STEP_VEHICLE_ID_LIST\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medgeID\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\domain.py:147\u001b[0m, in \u001b[0;36mDomain._getUniversal\u001b[1;34m(self, varID, objectID, format, *values)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deprecatedFor:\n\u001b[0;32m    146\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe domain \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is deprecated, use \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deprecatedFor))\n\u001b[1;32m--> 147\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _parse(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retValFunc, varID, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getCmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvarID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjectID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\domain.py:152\u001b[0m, in \u001b[0;36mDomain._getCmd\u001b[1;34m(self, varID, objID, format, *values)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FatalTraCIError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot connected.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 152\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendCmd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmdGetID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvarID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m r\u001b[38;5;241m.\u001b[39mreadLength()\n\u001b[0;32m    154\u001b[0m response, retVarID \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!BB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:231\u001b[0m, in \u001b[0;36mConnection._sendCmd\u001b[1;34m(self, cmdID, varID, objID, format, *values)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(objID)) \u001b[38;5;241m+\u001b[39m objID\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m packed\n\u001b[1;32m--> 231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendExact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:130\u001b[0m, in \u001b[0;36mConnection._sendExact\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _DEBUG:\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msending\u001b[39m\u001b[38;5;124m\"\u001b[39m, Storage(length \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string)\u001b[38;5;241m.\u001b[39mgetDebugString())\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recvExact()\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _DEBUG:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        'random_seed': 33,\n",
    "        'flow_on_HW': 5000,\n",
    "        'flow_on_Ramp': 2000,\n",
    "        'state_queue_len': 3,\n",
    "        'data_points': [\n",
    "            (0, 1000, 500), (10, 2000, 1300), (20, 3200, 1800),\n",
    "            (30, 2500, 1500), (40, 1500, 1000), (50, 1000, 700), (60, 800, 500)\n",
    "        ],\n",
    "        'simulation_step_len': 2,\n",
    "        'mu': 0.05,\n",
    "        'omega': -0.5,\n",
    "        'tau': 0.2,\n",
    "        'epochs': 70,\n",
    "        'batch_size': 32,\n",
    "        'max_steps': 1800,\n",
    "        'learning_rate': 5e-5,\n",
    "        'gamma': 0.99,\n",
    "        'eps_start': 0.8,\n",
    "        'eps_min': 0.05,\n",
    "        'eps_decay_factor': 0.05,\n",
    "        'eps_dec_exp': True,\n",
    "        'sync_freq': 5,\n",
    "        'mem_size': 50000\n",
    "    }\n",
    "\n",
    "    agent = SumoDQNAgent(action_space_n=2, observation_space_n=3012, config=config)\n",
    "    model, total_loss, total_rewards = agent.train()\n",
    "    torch.save(model.state_dict(), \"DynamicModel.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a DQN (Deep Q-Network) agent to train a neural network on a traffic simulation environment built with SUMO (Simulation of Urban MObility). The agent learns optimal actions to control traffic flow based on simulated traffic patterns.\n",
    "\n",
    "The primary components are:\n",
    "\n",
    "- Environment Interaction: Using a custom SumoEnv environment, the agent retrieves observations and performs actions.\n",
    "- Neural Network: A feedforward network trained via Q-learning.\n",
    "- Replay Buffer: Used to store and sample experience tuples.\n",
    "- Training with DQN: Using experience replay and a target network to improve stability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
